{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPEHg1p2XOKB"
      },
      "source": [
        "# Recognizing hand-written digits\n",
        "\n",
        "This example shows how scikit-learn can be used to recognize images of\n",
        "hand-written digits, from 0-9.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 837,
      "metadata": {
        "id": "Eru66-WIM6NY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 838,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXys0ZbYNomn",
        "outputId": "bf5d8cab-055b-458d-cb19-486dcd1a9c89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1437, 64), (360, 64), (1437, 10), (360, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 838
        }
      ],
      "source": [
        "dataset = load_digits()\n",
        "X = dataset.data\n",
        "Y = dataset.target\n",
        "Y = np.eye(10)[Y] # one hot\n",
        "\n",
        "X_train , X_test , Y_train , Y_test = train_test_split(X,Y,test_size=0.2)\n",
        "X_train.shape ,X_test.shape ,Y_train.shape ,Y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7SMO4MJ_upb"
      },
      "source": [
        "\n",
        "###  one hot\n",
        "\n",
        "---\n",
        "In short, with the following codes, we want to determine the number of outputs that we are considering here, which one is the new number from 0 to 9, that is, we have 10 outputs, the first of which shows the percentage of probability that it is zero, and the second of which is the percentage. The probability is one to the tenth, which means how many percent is the probability that the new number is not a number, but these are all probabilities. You cannot compare with the real number to see if it is correctly recognized or not, that's why the real number used for train is a number. For example, 5, for this we need to make that value one hot so that it becomes the same size as our output so that we can calculate the accuracy and other parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 852,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzVLTbvA9uxs",
        "outputId": "0b6003c1-6ade-4a66-9ed1-988f2d145aa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 852
        }
      ],
      "source": [
        "np.eye(9) #  هر عددی که هست رو یک میکند بقیه صفر\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 840,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFw5E-OsOGlt",
        "outputId": "aa5d8799-a4ed-471c-8304-230f595d7cc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 840
        }
      ],
      "source": [
        "dataset.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 841,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzAs7NqAOLgY",
        "outputId": "b42d4165-d92d-4a84-f192-d018e1b219b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 841
        }
      ],
      "source": [
        "dataset.images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 842,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZDa_S0FPGTn",
        "outputId": "079402ce-02d2-4030-e582-826285034771"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {},
          "execution_count": 842
        }
      ],
      "source": [
        "dataset.target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 843,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mpKtiuBPRsv",
        "outputId": "05cdf28a-7235-432d-f376-826c49f5cd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.  0.  0.  1. 14. 13.  1.  0.  0.  0.  0.  1. 16. 16.  3.  0.  0.  5.\n",
            " 11. 15. 16. 16.  0.  0.  0.  4. 15. 16. 16. 15.  0.  0.  0.  0.  0.  8.\n",
            " 16.  7.  0.  0.  0.  0.  0. 10. 16.  3.  0.  0.  0.  0.  0.  8. 16.  6.\n",
            "  0.  0.  0.  0.  0.  2. 13. 15.  2.  0.]\n",
            "[[ 0.  0.  0.  1. 14. 13.  1.  0.]\n",
            " [ 0.  0.  0.  1. 16. 16.  3.  0.]\n",
            " [ 0.  5. 11. 15. 16. 16.  0.  0.]\n",
            " [ 0.  4. 15. 16. 16. 15.  0.  0.]\n",
            " [ 0.  0.  0.  8. 16.  7.  0.  0.]\n",
            " [ 0.  0.  0. 10. 16.  3.  0.  0.]\n",
            " [ 0.  0.  0.  8. 16.  6.  0.  0.]\n",
            " [ 0.  0.  0.  2. 13. 15.  2.  0.]]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(dataset.data[80])\n",
        "print(dataset.images[80])\n",
        "print(dataset.target[80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 844,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "fSSGXsBiQi7u",
        "outputId": "6afd89a0-dba7-44ec-a4cf-47c16bed9e9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e943f2d7b20>"
            ]
          },
          "metadata": {},
          "execution_count": 844
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(dataset.images[80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 845,
      "metadata": {
        "id": "CSPkCdxWRqZw"
      },
      "outputs": [],
      "source": [
        "# Activation function\n",
        "def sigmoid(X):\n",
        "  return 1 / ( 1  + np.exp(-X))\n",
        "\n",
        "def softmax(X) :\n",
        "  return np.exp(X) / np.sum(np.exp(X))\n",
        "\n",
        "def root_mean_squired_error(Y_gt , Y_pred):\n",
        "  return np.sqrt(np.mean((Y_gt - Y_pred) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 846,
      "metadata": {
        "id": "k3CKDci2ZwPs"
      },
      "outputs": [],
      "source": [
        "epochs = 80\n",
        "lr = 0.001\n",
        "\n",
        "Dimension_Input = X_train.shape[1] # Dimension_Input = 64\n",
        "HiddenLayer1 = 128\n",
        "HiddenLayer2 = 32\n",
        "Dimension_Output = Y_train.shape[1] # Dimension_Output = len(np.unique(Y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 847,
      "metadata": {
        "id": "HDscnrL_896U"
      },
      "outputs": [],
      "source": [
        "W1 = np.random.randn(Dimension_Input , HiddenLayer1)\n",
        "W2 = np.random.randn(HiddenLayer1 , HiddenLayer2)\n",
        "W3 = np.random.randn(HiddenLayer2 , Dimension_Output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 848,
      "metadata": {
        "id": "FcsQwJnl8-qZ"
      },
      "outputs": [],
      "source": [
        "#  به ازای هر نرون در شبکه عصبی یک بایاس داریم\n",
        "bias1 = np.random.randn(HiddenLayer1)\n",
        "bias2 = np.random.randn(HiddenLayer2)\n",
        "bias3 = np.random.randn(Dimension_Output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 849,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncj4jANDHLdr",
        "outputId": "c78fb1fa-d27f-4eab-f3a7-6be0e5465059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss train: 0.30480130925639315 ---------------------- acc train: 0.24008350730688935\n",
            "loss test: 0.28679624081711874 ------------------------ acc test: 0.3194444444444444\n",
            "loss train: 0.26837195372677447 ---------------------- acc train: 0.441892832289492\n",
            "loss test: 0.26901286070538655 ------------------------ acc test: 0.41944444444444445\n",
            "loss train: 0.2460155561743052 ---------------------- acc train: 0.5629784272790536\n",
            "loss test: 0.25410682039673194 ------------------------ acc test: 0.49444444444444446\n",
            "loss train: 0.22917933227186932 ---------------------- acc train: 0.6360473208072372\n",
            "loss test: 0.24265372403443422 ------------------------ acc test: 0.5833333333333334\n",
            "loss train: 0.2163857059369593 ---------------------- acc train: 0.6896311760612387\n",
            "loss test: 0.23225069966701906 ------------------------ acc test: 0.6083333333333333\n",
            "loss train: 0.20514608379230784 ---------------------- acc train: 0.7313848295059151\n",
            "loss test: 0.22411791331346542 ------------------------ acc test: 0.6472222222222223\n",
            "loss train: 0.1958329166964925 ---------------------- acc train: 0.7606123869171886\n",
            "loss test: 0.2177444514185213 ------------------------ acc test: 0.6777777777777778\n",
            "loss train: 0.18721813235944776 ---------------------- acc train: 0.7800974251913709\n",
            "loss test: 0.21147184325751614 ------------------------ acc test: 0.6888888888888889\n",
            "loss train: 0.17921724495924304 ---------------------- acc train: 0.802366040361865\n",
            "loss test: 0.20615360222781004 ------------------------ acc test: 0.7\n",
            "loss train: 0.17218332434205696 ---------------------- acc train: 0.8162839248434238\n",
            "loss test: 0.2007382632296508 ------------------------ acc test: 0.7111111111111111\n",
            "loss train: 0.1657323016462583 ---------------------- acc train: 0.8322894919972165\n",
            "loss test: 0.1958329916111889 ------------------------ acc test: 0.7333333333333333\n",
            "loss train: 0.15973422652015098 ---------------------- acc train: 0.848295059151009\n",
            "loss test: 0.19165949757984027 ------------------------ acc test: 0.7416666666666667\n",
            "loss train: 0.15427658610252454 ---------------------- acc train: 0.8615170494084899\n",
            "loss test: 0.18804804043332493 ------------------------ acc test: 0.7472222222222222\n",
            "loss train: 0.1490818110408448 ---------------------- acc train: 0.8789144050104384\n",
            "loss test: 0.18474353644071784 ------------------------ acc test: 0.7611111111111111\n",
            "loss train: 0.14440576932055796 ---------------------- acc train: 0.8858733472512178\n",
            "loss test: 0.1819442473618238 ------------------------ acc test: 0.7805555555555556\n",
            "loss train: 0.14001344787265593 ---------------------- acc train: 0.8928322894919972\n",
            "loss test: 0.1793904668393191 ------------------------ acc test: 0.7861111111111111\n",
            "loss train: 0.13598810760983357 ---------------------- acc train: 0.8983994432846207\n",
            "loss test: 0.1768346944175064 ------------------------ acc test: 0.7972222222222223\n",
            "loss train: 0.13225534350750678 ---------------------- acc train: 0.9046624913013221\n",
            "loss test: 0.17421062414015034 ------------------------ acc test: 0.8027777777777778\n",
            "loss train: 0.12867561883782036 ---------------------- acc train: 0.9109255393180237\n",
            "loss test: 0.17209114925947083 ------------------------ acc test: 0.8027777777777778\n",
            "loss train: 0.1252609453042951 ---------------------- acc train: 0.9206680584551148\n",
            "loss test: 0.17011488160196472 ------------------------ acc test: 0.8055555555555556\n",
            "loss train: 0.12204524804911145 ---------------------- acc train: 0.9248434237995825\n",
            "loss test: 0.16808337382724897 ------------------------ acc test: 0.8138888888888889\n",
            "loss train: 0.11901251341616399 ---------------------- acc train: 0.9269311064718163\n",
            "loss test: 0.16615303843297632 ------------------------ acc test: 0.8222222222222222\n",
            "loss train: 0.11616050063368381 ---------------------- acc train: 0.929714683368128\n",
            "loss test: 0.1643621466833698 ------------------------ acc test: 0.825\n",
            "loss train: 0.11345478753816642 ---------------------- acc train: 0.9338900487125957\n",
            "loss test: 0.1627327421540162 ------------------------ acc test: 0.825\n",
            "loss train: 0.11087538606316444 ---------------------- acc train: 0.9366736256089074\n",
            "loss test: 0.1612390845025798 ------------------------ acc test: 0.8277777777777777\n",
            "loss train: 0.10846374841293836 ---------------------- acc train: 0.9394572025052192\n",
            "loss test: 0.15988266008383525 ------------------------ acc test: 0.8277777777777777\n",
            "loss train: 0.10620156230538488 ---------------------- acc train: 0.942240779401531\n",
            "loss test: 0.15860192054932082 ------------------------ acc test: 0.8277777777777777\n",
            "loss train: 0.10405700916907562 ---------------------- acc train: 0.9491997216423104\n",
            "loss test: 0.15734035145538805 ------------------------ acc test: 0.8277777777777777\n",
            "loss train: 0.1019962478191769 ---------------------- acc train: 0.9498956158663883\n",
            "loss test: 0.15607808177507623 ------------------------ acc test: 0.8305555555555556\n",
            "loss train: 0.10001190909754774 ---------------------- acc train: 0.9526791927627001\n",
            "loss test: 0.15484255556856288 ------------------------ acc test: 0.8388888888888889\n",
            "loss train: 0.09812348214306611 ---------------------- acc train: 0.9547668754349339\n",
            "loss test: 0.15365323734608682 ------------------------ acc test: 0.8444444444444444\n",
            "loss train: 0.09633018726610025 ---------------------- acc train: 0.9561586638830898\n",
            "loss test: 0.15253995239279794 ------------------------ acc test: 0.85\n",
            "loss train: 0.09461355345276597 ---------------------- acc train: 0.9589422407794015\n",
            "loss test: 0.1515092383444098 ------------------------ acc test: 0.8555555555555555\n",
            "loss train: 0.09297230045256485 ---------------------- acc train: 0.9610299234516354\n",
            "loss test: 0.15055348293880252 ------------------------ acc test: 0.8555555555555555\n",
            "loss train: 0.09139391383925201 ---------------------- acc train: 0.9617258176757133\n",
            "loss test: 0.14966748404229796 ------------------------ acc test: 0.8583333333333333\n",
            "loss train: 0.08990598349816303 ---------------------- acc train: 0.9638135003479471\n",
            "loss test: 0.14885474250896819 ------------------------ acc test: 0.8583333333333333\n",
            "loss train: 0.08851198800618873 ---------------------- acc train: 0.9638135003479471\n",
            "loss test: 0.14810562284187287 ------------------------ acc test: 0.8583333333333333\n",
            "loss train: 0.08719119816297564 ---------------------- acc train: 0.964509394572025\n",
            "loss test: 0.1473971357608108 ------------------------ acc test: 0.8583333333333333\n",
            "loss train: 0.08592798752779375 ---------------------- acc train: 0.964509394572025\n",
            "loss test: 0.14670385996129442 ------------------------ acc test: 0.8611111111111112\n",
            "loss train: 0.08470516872163153 ---------------------- acc train: 0.965205288796103\n",
            "loss test: 0.14600948482468848 ------------------------ acc test: 0.8638888888888889\n",
            "loss train: 0.08351922695003793 ---------------------- acc train: 0.9665970772442589\n",
            "loss test: 0.14532683660316678 ------------------------ acc test: 0.8694444444444445\n",
            "loss train: 0.08238318581616942 ---------------------- acc train: 0.9686847599164927\n",
            "loss test: 0.1446829756501727 ------------------------ acc test: 0.8694444444444445\n",
            "loss train: 0.08129965146226856 ---------------------- acc train: 0.9693806541405706\n",
            "loss test: 0.14408035077874026 ------------------------ acc test: 0.8722222222222222\n",
            "loss train: 0.08026347008362571 ---------------------- acc train: 0.9693806541405706\n",
            "loss test: 0.1435192517164571 ------------------------ acc test: 0.8722222222222222\n",
            "loss train: 0.07926799212482824 ---------------------- acc train: 0.9693806541405706\n",
            "loss test: 0.14299536577958044 ------------------------ acc test: 0.8722222222222222\n",
            "loss train: 0.07830296465489188 ---------------------- acc train: 0.9707724425887265\n",
            "loss test: 0.14249686153011992 ------------------------ acc test: 0.875\n",
            "loss train: 0.07736012818158473 ---------------------- acc train: 0.9707724425887265\n",
            "loss test: 0.14201847964980724 ------------------------ acc test: 0.875\n",
            "loss train: 0.07643601133659071 ---------------------- acc train: 0.9735560194850382\n",
            "loss test: 0.14156487335573797 ------------------------ acc test: 0.875\n",
            "loss train: 0.07552549349008653 ---------------------- acc train: 0.9735560194850382\n",
            "loss test: 0.14112206670108363 ------------------------ acc test: 0.875\n",
            "loss train: 0.07461641101481911 ---------------------- acc train: 0.9735560194850382\n",
            "loss test: 0.1406631186154828 ------------------------ acc test: 0.8777777777777778\n",
            "loss train: 0.07368543493976575 ---------------------- acc train: 0.9735560194850382\n",
            "loss test: 0.14019691851358448 ------------------------ acc test: 0.8777777777777778\n",
            "loss train: 0.0727320906866975 ---------------------- acc train: 0.9770354906054279\n",
            "loss test: 0.13974987554891058 ------------------------ acc test: 0.8777777777777778\n",
            "loss train: 0.07180901626465833 ---------------------- acc train: 0.9770354906054279\n",
            "loss test: 0.13932075670075647 ------------------------ acc test: 0.8777777777777778\n",
            "loss train: 0.07092494324591936 ---------------------- acc train: 0.977731384829506\n",
            "loss test: 0.13890098185338398 ------------------------ acc test: 0.8833333333333333\n",
            "loss train: 0.07006507239372381 ---------------------- acc train: 0.9784272790535838\n",
            "loss test: 0.13849201742533926 ------------------------ acc test: 0.8861111111111111\n",
            "loss train: 0.0692241061841665 ---------------------- acc train: 0.9784272790535838\n",
            "loss test: 0.1381056309453354 ------------------------ acc test: 0.8861111111111111\n",
            "loss train: 0.06840382948593078 ---------------------- acc train: 0.9784272790535838\n",
            "loss test: 0.13774571783028947 ------------------------ acc test: 0.8861111111111111\n",
            "loss train: 0.06760452304788978 ---------------------- acc train: 0.9791231732776617\n",
            "loss test: 0.13741312792601518 ------------------------ acc test: 0.8861111111111111\n",
            "loss train: 0.0668244076940135 ---------------------- acc train: 0.9791231732776617\n",
            "loss test: 0.1371096016583384 ------------------------ acc test: 0.8861111111111111\n",
            "loss train: 0.06605750741807796 ---------------------- acc train: 0.9798190675017397\n",
            "loss test: 0.13683789283625544 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.06529712143187584 ---------------------- acc train: 0.9798190675017397\n",
            "loss test: 0.1366046032271639 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.06454706769343664 ---------------------- acc train: 0.9805149617258176\n",
            "loss test: 0.13641288794793138 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.06380691648586612 ---------------------- acc train: 0.9819067501739736\n",
            "loss test: 0.13625905994826265 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.06306768090508671 ---------------------- acc train: 0.9819067501739736\n",
            "loss test: 0.13612541291658636 ------------------------ acc test: 0.8833333333333333\n",
            "loss train: 0.06232750826325459 ---------------------- acc train: 0.9819067501739736\n",
            "loss test: 0.13597688642593272 ------------------------ acc test: 0.8833333333333333\n",
            "loss train: 0.061592017720863486 ---------------------- acc train: 0.9826026443980515\n",
            "loss test: 0.13578036128527599 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.060898175644471274 ---------------------- acc train: 0.9826026443980515\n",
            "loss test: 0.13555169086554164 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.060251831444807065 ---------------------- acc train: 0.9839944328462074\n",
            "loss test: 0.13531764943814614 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.05964438351263969 ---------------------- acc train: 0.9839944328462074\n",
            "loss test: 0.13508474180546354 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.059061698057564546 ---------------------- acc train: 0.9839944328462074\n",
            "loss test: 0.13485350702113344 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.0584957177781249 ---------------------- acc train: 0.9839944328462074\n",
            "loss test: 0.13462150035027473 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.05794537011509541 ---------------------- acc train: 0.9846903270702854\n",
            "loss test: 0.13438384277137863 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.057410030273707856 ---------------------- acc train: 0.9846903270702854\n",
            "loss test: 0.134134672662354 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.05688554488074667 ---------------------- acc train: 0.9846903270702854\n",
            "loss test: 0.13387124762544747 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.056364543912835706 ---------------------- acc train: 0.9846903270702854\n",
            "loss test: 0.13360693107218707 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.05583817808836254 ---------------------- acc train: 0.9853862212943633\n",
            "loss test: 0.13338025537603745 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.055313448822762006 ---------------------- acc train: 0.9860821155184412\n",
            "loss test: 0.13320871311914304 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.05481100453913394 ---------------------- acc train: 0.9874739039665971\n",
            "loss test: 0.133065802173749 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.05433025647559338 ---------------------- acc train: 0.988169798190675\n",
            "loss test: 0.13292834528074773 ------------------------ acc test: 0.8805555555555555\n",
            "loss train: 0.053865295613968946 ---------------------- acc train: 0.988169798190675\n",
            "loss test: 0.13279051909841127 ------------------------ acc test: 0.8805555555555555\n",
            "completed!\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    # train\n",
        "\n",
        "    Y_pred_train = []\n",
        "    for x, y in zip(X_train, Y_train):\n",
        "\n",
        "        # forward\n",
        "        x = x.reshape(-1, 1)\n",
        "\n",
        "        # layer 1\n",
        "        net1 = x.T @ W1 + bias1\n",
        "        out1 = sigmoid(net1)\n",
        "\n",
        "        # layer 2\n",
        "        net2 = out1 @ W2 + bias2\n",
        "        out2 = sigmoid(net2)\n",
        "\n",
        "        # layer 3\n",
        "        net3 = out2 @ W3 + bias3\n",
        "        out3 = softmax(net3)\n",
        "\n",
        "        y_pred = out3\n",
        "        Y_pred_train.append(y_pred.T)\n",
        "\n",
        "        # back propagation\n",
        "\n",
        "        # layer 3\n",
        "        error = -2 * (y - y_pred)\n",
        "        Gradient_W3 = out2.T @ error\n",
        "        Gradient_bias3 = error\n",
        "\n",
        "        # layer 2\n",
        "        error = error @ W3.T * out2 * (1 - out2)\n",
        "        Gradient_W2 = out1.T @ error\n",
        "        Gradient_bias2 = error\n",
        "\n",
        "        # layer 1\n",
        "        error = error @ W2.T * out1 * (1 - out1)\n",
        "        Gradient_W1 = x @ error\n",
        "        Gradient_bias1 = error\n",
        "\n",
        "        # update\n",
        "\n",
        "        # layer 1\n",
        "        W1 = W1 - lr * Gradient_W1\n",
        "        bias1 = bias1 - lr * Gradient_bias1\n",
        "\n",
        "        # layer 2\n",
        "        W2 = W2 - lr * Gradient_W2\n",
        "        bias2 = bias2 - lr * Gradient_bias2\n",
        "\n",
        "        # layer 3\n",
        "        W3 = W3 - lr * Gradient_W3\n",
        "        bias3 = bias3 - lr * Gradient_bias3\n",
        "\n",
        "    Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n",
        "    loss_train = root_mean_squired_error(Y_pred_train, Y_train)\n",
        "    acc_train = np.mean(np.argmax(Y_pred_train, axis=1) == np.argmax(Y_train, axis=1))\n",
        "\n",
        "    # test\n",
        "\n",
        "    Y_pred_test = []\n",
        "    for x, y in zip(X_test, Y_test):\n",
        "\n",
        "        # forward\n",
        "        x = x.reshape(-1, 1)\n",
        "\n",
        "        # layer 1\n",
        "        net1 = x.T @ W1 + bias1\n",
        "        out1 = sigmoid(net1)\n",
        "\n",
        "        # layer 2\n",
        "        net2 = out1 @ W2 + bias2\n",
        "        out2 = sigmoid(net2)\n",
        "\n",
        "        # layer 3\n",
        "        net3 = out2 @ W3 + bias3\n",
        "        out3 = softmax(net3)\n",
        "\n",
        "        y_pred = out3\n",
        "        Y_pred_test.append(y_pred.T)\n",
        "\n",
        "    Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n",
        "    loss_test = root_mean_squired_error(Y_pred_test, Y_test)\n",
        "    acc_test = np.mean(np.argmax(Y_pred_test, axis=1) == np.argmax(Y_test, axis=1))\n",
        "\n",
        "    print('loss train:', loss_train,\"----------------------\" ,'acc train:', acc_train)\n",
        "    print('loss test:', loss_test,\"------------------------\" ,'acc test:', acc_test)\n",
        "\n",
        "print('completed!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 850,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk1baoynWG0O",
        "outputId": "8c385b78-9998-48b3-eef9-89b3c5cfe29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "image = cv2.imread(\"/content/input/0.jpg\")\n",
        "image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "image = image.reshape(64,1)\n",
        "x = image\n",
        "\n",
        "# forward\n",
        "\n",
        "# layer 1\n",
        "net1 = x.T @ W1 + bias1\n",
        "out1 = sigmoid(net1)\n",
        "\n",
        "# layer 2\n",
        "net2 = out1 @ W2 + bias2\n",
        "out2 = sigmoid(net2)\n",
        "\n",
        "# layer 3\n",
        "net3 = out2 @ W3 + bias3\n",
        "out3 = softmax(net3)\n",
        "\n",
        "y_pred = out3\n",
        "print(np.argmax(y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    image = cv2.imread(f\"input/{i}.jpg\")\n",
        "    image = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    if i==0:\n",
        "        image1=image\n",
        "\n",
        "    image = image.reshape(1,64)\n",
        "\n",
        "    x= image\n",
        "    # forward\n",
        "\n",
        "    # layer 1\n",
        "\n",
        "    out1 = sigmoid(x @ W1 + bias1)\n",
        "\n",
        "    # layer 2\n",
        "    net2 = out1 @ W2 + bias2\n",
        "    out2 = sigmoid(net2)\n",
        "\n",
        "    # layer 3\n",
        "    net3 = out2 @ W3 + bias3\n",
        "    out3 = softmax(net3)\n",
        "\n",
        "    y_pred = out3\n",
        "    y_predict = np.argmax(y_pred)\n",
        "    # print(f\"result: {y_pred}\")\n",
        "    print(f\"True number is: {i}  , predict number is:{y_predict}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqE6vS_0-ZfM",
        "outputId": "a14e4ae6-17f7-4f99-f3b7-b93dae87e03b"
      },
      "execution_count": 851,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True number is: 0  , predict number is:0\n",
            "True number is: 1  , predict number is:1\n",
            "True number is: 2  , predict number is:2\n",
            "True number is: 3  , predict number is:3\n",
            "True number is: 4  , predict number is:7\n",
            "True number is: 5  , predict number is:5\n",
            "True number is: 6  , predict number is:8\n",
            "True number is: 7  , predict number is:7\n",
            "True number is: 8  , predict number is:5\n",
            "True number is: 9  , predict number is:9\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}